distributed_type: FSDP
mixed_precision: "bf16"
# mixed_precision: "fp8"
# fp8_config:
#   backend: TE # Can be TE | MS-AMP
#   # The following are TE specific arguments.
#   # See https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html#common-api for more details
#   amax_history_len: 1024
#   fp8_format: E4M3
#   interval: 1
#   margin: 0
#   override_linear_precision: [false, false, false]
#   # Generally this should always be set to `false` to have the most realistic fp8 eval performance
#   use_autocast_during_eval: false
#   # If using MS-AMP, we ignore all of the prior and set a opt_level
#   #opt_level: O1
fsdp_config:
  fsdp_activation_checkpointing: false
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: false
  fsdp_forward_prefetch: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: LLaDALlamaBlock
  fsdp_use_orig_params: true
  fsdp_ignored_modules: null
